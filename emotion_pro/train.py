import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from torch.nn.modules.loss import _WeightedLoss # ğŸ”¥ Loss ê°€ì¤‘ì¹˜ ì‚¬ìš©ì„ ìœ„í•´ ì„í¬íŠ¸
from sklearn.metrics import accuracy_score

# ğŸ”¥ Custom Weighted Cross Entropy Loss
class WeightedCrossEntropyLoss(_WeightedLoss):
    def forward(self, input, target, weight=None):
        # reduction='none'ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë°°ì¹˜ í¬ê¸°ë§Œí¼ì˜ Loss ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.
        ce_loss = F.cross_entropy(input, target, reduction='none')
        
        if weight is not None:
            # Loss ë²¡í„°ì— ìƒ˜í”Œë³„ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•©ë‹ˆë‹¤.
            ce_loss = ce_loss * weight

        # í‰ê·  Lossë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return ce_loss.mean()


# run_epoch í•¨ìˆ˜ ìˆ˜ì •: weightë¥¼ ë°›ê³ , WeightedCrossEntropyLoss ì‚¬ìš©
def run_epoch(model, loader, optimizer, device, mode="train"):
    if mode == "train":
        model.train()
    else:
        model.eval()

    total, correct = 0, 0
    total_loss = 0
    # ğŸ”¥ nn.CrossEntropyLoss() ëŒ€ì‹  Custom Loss ì‚¬ìš©
    ce = WeightedCrossEntropyLoss()

    loader_pbar = tqdm(loader, desc=f"{mode.upper()}", leave=False)

    with torch.set_grad_enabled(mode == "train") and torch.autograd.set_detect_anomaly(False):
        # ğŸ”¥ weight ë³€ìˆ˜ ì¶”ê°€
        for text_input, bio_input, label, weight in loader_pbar:
            
            for k in text_input:
                text_input[k] = text_input[k].to(device)
            bio_input = bio_input.to(device)
            label = label.to(device)
            # ğŸ”¥ weightë¥¼ ì¥ì¹˜ë¡œ ì´ë™
            weight = weight.to(device) 

            if mode == "train":
                optimizer.zero_grad()

            logits = model(text_input, bio_input)
            # ğŸ”¥ loss ê³„ì‚° ì‹œ weight ì „ë‹¬
            loss = ce(logits, label, weight=weight) 

            if mode == "train":
                loss.backward()
                optimizer.step()

            pred = logits.argmax(dim=1)
            correct += (pred == label).sum().item()
            total += label.size(0)
            total_loss += loss.item()
            
            current_acc = correct / total
            loader_pbar.set_postfix({'loss': loss.item(), 'acc': current_acc})

    return total_loss / len(loader), current_acc


# test_multimodal í•¨ìˆ˜ ìˆ˜ì •: ëª¨ë‹¬ë¦¬í‹° Zero-out ë¡œì§ êµ¬í˜„
def test_multimodal(model, loader, device, shuffle_mode="none"):
    """
    ëª¨ë‹¬ë¦¬í‹° Zero-outì„ ìœ„í•œ test í•¨ìˆ˜
    shuffle_mode: "none" (ì •ìƒ), "text_zeroout" (Bio-Only), "bio_zeroout" (Text-Only)
    Returns: acc, loss, labels, preds, probs
    """
    model.eval()
    total_loss = 0
    ce = nn.CrossEntropyLoss() # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì¼ë°˜ CrossEntropyLoss ì‚¬ìš©
    
    all_preds = []
    all_labels = []
    all_probs = []

    loader_pbar = tqdm(loader, desc=f"TEST({shuffle_mode})", leave=True)

    with torch.no_grad():
        # ğŸ”¥ test ì‹œì—ëŠ” weightê°€ í•„ìš” ì—†ìœ¼ë‚˜, Dataset ë°˜í™˜ í˜•ì‹ì— ë§ì¶° ë°›ì•„ì˜µë‹ˆë‹¤.
        for text_input, bio_input, label, _ in loader_pbar: 
            
            for k in text_input:
                text_input[k] = text_input[k].to(device)
            bio_input = bio_input.to(device)
            label = label.to(device)

            # ğŸ”¥ Zero-out ë¡œì§ êµ¬í˜„
            if shuffle_mode == "text_zeroout":
                # í…ìŠ¤íŠ¸ ì •ë³´ ë§ˆìŠ¤í‚¹: input_idsì™€ attention_maskë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                # KoBERTê°€ frozen ìƒíƒœì´ë¯€ë¡œ, inputì´ 0ì´ë©´ íŠ¹ì • ê³ ì •ëœ ë²¡í„°ê°€ ë‚˜ì˜¬ ê²ƒì„
                # (ë˜ëŠ” ë‹¨ìˆœíˆ KoBERTì˜ ê²°ê³¼ì¸ text_featë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‚˜,
                #  ì—¬ê¸°ì„œëŠ” inputì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.)
                for k in text_input:
                     text_input[k].zero_() # ëª¨ë“  ì›ì†Œë¥¼ 0ìœ¼ë¡œ 

            elif shuffle_mode == "bio_zeroout":
                # Bio ì •ë³´ ë§ˆìŠ¤í‚¹: Bio input ë²¡í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                bio_input.zero_() # ëª¨ë“  ì›ì†Œë¥¼ 0ìœ¼ë¡œ

            logits = model(text_input, bio_input)
            loss = ce(logits, label)
            total_loss += loss.item()

            # AUC ê³„ì‚°ìš© í™•ë¥ ê°’ (ì´ì§„ ë¶„ë¥˜ ì‹œ 1ë²ˆ í´ë˜ìŠ¤ í™•ë¥ )
            probs = F.softmax(logits, dim=1)[:, 1] 
            pred = logits.argmax(dim=1)
            
            all_preds.extend(pred.cpu().tolist())
            all_labels.extend(label.cpu().tolist())
            all_probs.extend(probs.cpu().tolist())

    acc = accuracy_score(all_labels, all_preds)
    avg_loss = total_loss / len(loader)
    
    return acc, avg_loss, all_labels, all_preds, all_probs