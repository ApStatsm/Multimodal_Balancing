# dataset.py
import os
import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from kobert_tokenizer import KoBERTTokenizer
import chardet
import re

# ===============================
# KoBERT Dataset Class
# ===============================
class KoBERTDataset(Dataset):
    printed_label_map = False

    def __init__(self, df, text_folder, tokenizer, max_len=128):
        self.df = df.reset_index(drop=True)
        self.text_folder = text_folder
        self.tokenizer = tokenizer
        self.max_len = max_len

        self.texts = []
        self.labels = []

        # ê³ ì •ëœ ê°ì • ë§¤í•‘
        self.label_map = {
            "neutral": 0,
            "surprise": 1,
            "angry": 2,
            "sad": 3,
            "happy": 4
        }

        # ë¼ë²¨ ë§¤í•‘ ë¡œê·¸ í•œ ë²ˆë§Œ ì¶œë ¥
        if not KoBERTDataset.printed_label_map:
            print(" Using predefined label mapping:")
            for k, v in self.label_map.items():
                print(f"  {k:<15} â†’ {v}")
            KoBERTDataset.printed_label_map = True

        #  Segment_ID ë‹¨ìœ„ë¡œ txt 1ê°œë§Œ ë¡œë“œ
        for _, row in self.df.iterrows():
            seg_id = str(row["Segment_ID"]).strip()
            label_name = row["Emotion"]

            # ë¼ë²¨ ì¸ë±ìŠ¤ ë³€í™˜
            label = self.label_map.get(label_name, 0)

            # txt íŒŒì¼ ê²½ë¡œ íƒìƒ‰
            txt_path = None
            for root, _, files in os.walk(text_folder):
                if f"{seg_id}.txt" in files:
                    txt_path = os.path.join(root, f"{seg_id}.txt")
                    break

            # íŒŒì¼ ë¡œë“œ
            if txt_path:
                try:
                    with open(txt_path, "rb") as f:
                        raw = f.read()
                        enc = chardet.detect(raw)["encoding"] or "cp949"
                    text = raw.decode(enc, errors="ignore").strip()
                    if not text:
                        print(f" {seg_id}.txt ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f" {txt_path} ì½ê¸° ì‹¤íŒ¨ ({e})")
                    text = ""
            else:
                print(f" {seg_id}.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                text = ""

            # ğŸ‘‡ [ìˆ˜ì • í›„] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
            # 1. í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸(.,?!)ë¥¼ ì œì™¸í•œ ì¡ë™ì‚¬ë‹ˆ ì œê±°
            clean_text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s\.\,\?\!]", "", text)
            
            # 2. ë¶ˆí•„ìš”í•œ ë‹¤ì¤‘ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
            clean_text = re.sub(r"\s+", " ", clean_text).strip()

            self.texts.append(clean_text)
            self.labels.append(label)

    # í•„ìˆ˜ ë©”ì„œë“œ (DataLoaderìš©)
    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "token_type_ids": encoding["token_type_ids"].squeeze(0),
            "label": torch.tensor(label, dtype=torch.long),
            "text": text
        }


# ===============================
#  Data Load Function
# ===============================
def load_data_from_folders(tokenizer, csv_path, text_folder,
                           test_size=0.1, val_size=0.1, batch_size=16, max_len=128):

    all_dfs = []

    #  CSV ê²½ë¡œê°€ í´ë”ì¸ì§€ í™•ì¸
    if os.path.isdir(csv_path):
        csv_files = [f for f in os.listdir(csv_path) if f.endswith(".csv")]
        print(f" CSV folder detected: {csv_path}")
        print(f" Found {len(csv_files)} CSV files: {csv_files}")

        for fname in csv_files:
            full_path = os.path.join(csv_path, fname)
            with open(full_path, "rb") as f:
                raw = f.read()
                enc = chardet.detect(raw)["encoding"] or "cp949"

            try:
                df = pd.read_csv(full_path, encoding=enc)
                df.columns = df.columns.str.strip().str.replace("\ufeff", "", regex=True)
                all_dfs.append(df)
                print(f" Loaded {fname} ({len(df)} rows)")
            except Exception as e:
                print(f"ï¸ Failed to read {fname}: {e}")

    else:
        #  ë‹¨ì¼ CSV íŒŒì¼ ì²˜ë¦¬
        with open(csv_path, "rb") as f:
            raw = f.read()
            enc = chardet.detect(raw)["encoding"] or "cp949"
        df = pd.read_csv(csv_path, encoding=enc)
        df.columns = df.columns.str.strip().str.replace("\ufeff", "", regex=True)
        all_dfs.append(df)
        print(f" Loaded single CSV: {csv_path} ({len(df)} rows)")

    #  ì—¬ëŸ¬ CSV í•©ì¹˜ê¸°
    if len(all_dfs) > 0:
        df = pd.concat(all_dfs, ignore_index=True)
        print(f" Combined total rows: {len(df)}")
    else:
        raise ValueError(" No CSV files found in the specified path.")

        #  Segment_ID ê¸°ì¤€ ê·¸ë£¹í™”
    grouped = (
        df.groupby("Segment_ID")["Emotion"]
        .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])
        .reset_index()
    )

    print(f"Aggregated by Segment_ID â†’ {len(grouped)} unique segments")
    print(" Emotion distribution:\n", grouped["Emotion"].value_counts())

    # ===============================
    #  8 : 2 ë¹„ìœ¨ë¡œ Train / Test ë¶„í• 
    # ===============================
    train_df, test_df = train_test_split(
        grouped,
        test_size=0.2,  # ğŸ”¸ 20% = test
        stratify=grouped["Emotion"],  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
        random_state=42
    )

    print(f" Dataset split (8:2): Train {len(train_df)}, Test {len(test_df)}")

    #  DataFrame â†’ Dataset
    train_set = KoBERTDataset(train_df, text_folder, tokenizer, max_len)
    test_set = KoBERTDataset(test_df, text_folder, tokenizer, max_len)

    # DataLoader ìƒì„±
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_set, batch_size=batch_size)

    # âœ… ì´ì œëŠ” train, testë§Œ ë°˜í™˜
    return train_loader, test_loader