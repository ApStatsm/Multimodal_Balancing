from config import config
from utils import get_device
from dataset_multimodal import load_multimodal_data
from models.multimodal_e2e import MultimodalEndToEnd
from train import run_epoch, evaluate_with_shuffle 

import torch
import torch.nn as nn
# ğŸ”¥ [ìˆ˜ì • 1] train_test_split ì¶”ê°€
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import confusion_matrix
from torch.utils.data import DataLoader, SubsetRandomSampler
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ==========================================
# ğŸ“Š í˜¼ë™ í–‰ë ¬ ê·¸ë¦¬ê¸° ë„ìš°ë¯¸ í•¨ìˆ˜
# ==========================================
def save_confusion_matrix(y_true, y_pred, title, filename):
    labels = ["Neutral", "Biased"]
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                xticklabels=labels, yticklabels=labels)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(filename)
    plt.close() # ë©”ëª¨ë¦¬ í•´ì œ
    print(f"   >> ğŸ’¾ Saved: {filename}")

# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================
def main():
    # 1. ì´ˆê¸° ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
    tokenizer = None
    from kobert_tokenizer import KoBERTTokenizer
    tokenizer = KoBERTTokenizer.from_pretrained("skt/kobert-base-v1")
    device = get_device()
    criterion = nn.CrossEntropyLoss()

    config["model"]["num_classes"] = 2
    print(f"ğŸ”§ Config Update: num_classes = {config['model']['num_classes']} (Binary: Neutral vs Biased)")

    full_loader, _, _ = load_multimodal_data(
        tokenizer=tokenizer,
        session_folder=config["paths"]["session_folder"],
        text_folder=config["paths"]["text_folder"],
        batch_size=config["training"]["batch_size"],
        max_len=config["model"]["max_len"]
    )
    full_dataset = full_loader.dataset

    # ì „ì²´ ì¸ë±ìŠ¤ì™€ ë¼ë²¨ ì¶”ì¶œ
    all_indices = np.arange(len(full_dataset))
    all_labels = np.array([full_dataset[i][2].item() for i in range(len(full_dataset))])
    
    print(f"\nğŸ“¦ Total Data Samples: {len(all_indices)}")

    # ---------------------------------------------------------
    # ğŸ”¥ [ìˆ˜ì • 2] Test Set (20%) ì˜êµ¬ ê²©ë¦¬
    # ---------------------------------------------------------
    # dev_idx (80%): í•™ìŠµ(60%) + ê²€ì¦(20%)ì— ì‚¬ìš©
    # test_idx (20%): ìµœì¢… í‰ê°€ì—ë§Œ ì‚¬ìš© (LOCKED)
    dev_idx, test_idx = train_test_split(
        all_indices, 
        test_size=0.2, 
        stratify=all_labels, 
        random_state=42
    )
    
    # ê²©ë¦¬ëœ Testìš© Loader ìƒì„±
    test_subsampler = SubsetRandomSampler(test_idx)
    final_test_loader = DataLoader(full_dataset, batch_size=config["training"]["batch_size"], sampler=test_subsampler)
    
    print(f"   ğŸ”¹ Dev Set (For CV) : {len(dev_idx)} samples (80%)")
    print(f"   ğŸ”¹ Final Test Set   : {len(test_idx)} samples (20%) - LOCKED ğŸ”’")

    # ---------------------------------------------------------
    # ğŸ”¥ [ìˆ˜ì • 3] 4-Fold Cross Validation (ë‚¨ì€ 80%ì— ëŒ€í•´ ìˆ˜í–‰)
    # ---------------------------------------------------------
    # Dev(80%)ë¥¼ 4ë“±ë¶„í•˜ë©´ -> 1ì¡°ê°ì€ 20%ê°€ ë¨.
    # ì¦‰, Train(3ì¡°ê°=60%) : Val(1ì¡°ê°=20%) ë¹„ìœ¨ ì™„ì„±
    n_splits = 4 
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    # Dev ì…‹ì˜ ë¼ë²¨ë§Œ ë”°ë¡œ ì¶”ì¶œ (Stratifiedë¥¼ ìœ„í•´)
    dev_labels = all_labels[dev_idx]
    
    # ê²°ê³¼ ì €ì¥ì†Œ
    data_storage = {
        "normal": {"true": [], "pred": [], "acc": []},
        "bio_shuffled": {"true": [], "pred": [], "acc": []},
        "text_shuffled": {"true": [], "pred": [], "acc": []}
    }

    print(f"\n================ STARTING {n_splits}-FOLD CV ON DEV SET ================\n")
    print("   Target Split Ratio -> Train: 60% | Val: 20% | Test: 20%")

    # dev_idxë¥¼ ê°€ì§€ê³  K-Foldë¥¼ ë•ë‹ˆë‹¤.
    for fold_idx, (inner_train_idx, inner_val_idx) in enumerate(skf.split(dev_idx, dev_labels)):
        print(f"\n----- Fold {fold_idx+1} / {n_splits} -----")
        
        # skfê°€ ë±‰ëŠ” ê±´ dev_idx ë‚´ë¶€ì˜ 'ìƒëŒ€ì  ìœ„ì¹˜'ì´ë¯€ë¡œ, 'ì ˆëŒ€ ì¸ë±ìŠ¤'ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.
        real_train_idx = dev_idx[inner_train_idx]
        real_val_idx   = dev_idx[inner_val_idx]
        
        train_subsampler = SubsetRandomSampler(real_train_idx)
        val_subsampler   = SubsetRandomSampler(real_val_idx) # Early Stoppingìš©
        
        train_loader = DataLoader(full_dataset, batch_size=config["training"]["batch_size"], sampler=train_subsampler)
        val_loader   = DataLoader(full_dataset, batch_size=config["training"]["batch_size"], sampler=val_subsampler)
        
        model = MultimodalEndToEnd(config).to(device)
        optimizer = torch.optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()), 
            lr=config["training"]["learning_rate"]
        )

        # --- í•™ìŠµ ë£¨í”„ ---
        best_val_loss = float('inf')
        patience = 3
        counter = 0
        best_model_state = None
        
        for epoch in range(config["training"]["epochs"]):
            # Train (60% ë°ì´í„° ì‚¬ìš©)
            train_acc, train_loss, _ = run_epoch(model, train_loader, optimizer, device, mode="train")
            # Val (20% ë°ì´í„° ì‚¬ìš© - Early Stopping ì²´í¬)
            val_acc, val_loss, _ = run_epoch(model, val_loader, optimizer, device, mode="val")
            
            print(f"Ep {epoch+1:02d} | Train: {train_acc:.4f} ({train_loss:.4f}) | Val: {val_acc:.4f} ({val_loss:.4f})")
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_state = model.state_dict()
                counter = 0
            else:
                counter += 1
                if counter >= patience:
                    print(f"   >> ğŸ›‘ Early Stopping (Best Val Loss: {best_val_loss:.4f})")
                    break
        
        # --- ìµœì¢… í‰ê°€ (ğŸ”¥ ê²©ë¦¬í•´ë‘” 20% Test Set ì‚¬ìš©) ---
        if best_model_state:
            model.load_state_dict(best_model_state)
            
        # 1. Normal (ì •ìƒ)
        acc, _, t, p = evaluate_with_shuffle(model, final_test_loader, device, criterion, shuffle_bio=False, shuffle_text=False)
        data_storage["normal"]["acc"].append(acc)
        data_storage["normal"]["true"].extend(t)
        data_storage["normal"]["pred"].extend(p)

        # 2. Bio Shuffled (í…ìŠ¤íŠ¸ë§Œ ì •ìƒ -> í…ìŠ¤íŠ¸ ì˜ì¡´ë„ í™•ì¸)
        acc, _, t, p = evaluate_with_shuffle(model, final_test_loader, device, criterion, shuffle_bio=True, shuffle_text=False)
        data_storage["bio_shuffled"]["acc"].append(acc)
        data_storage["bio_shuffled"]["true"].extend(t)
        data_storage["bio_shuffled"]["pred"].extend(p)

        # 3. Text Shuffled (ë°”ì´ì˜¤ë§Œ ì •ìƒ -> ë°”ì´ì˜¤ ì˜ì¡´ë„ í™•ì¸)
        acc, _, t, p = evaluate_with_shuffle(model, final_test_loader, device, criterion, shuffle_bio=False, shuffle_text=True)
        data_storage["text_shuffled"]["acc"].append(acc)
        data_storage["text_shuffled"]["true"].extend(t)
        data_storage["text_shuffled"]["pred"].extend(p)

        print(f"   ğŸ‘‰ [Final Test Set Result] Normal: {data_storage['normal']['acc'][-1]:.4f}")

    # ---------------------------------------------------------
    # 4. ìµœì¢… ê²°ê³¼ ìš”ì•½
    # ---------------------------------------------------------
    print("\n================ FINAL SUMMARY (Test Set) ================\n")
    
    avg_norm = np.mean(data_storage["normal"]["acc"])
    avg_bio_shuf = np.mean(data_storage["bio_shuffled"]["acc"])
    avg_text_shuf = np.mean(data_storage["text_shuffled"]["acc"])
    
    print(f"1ï¸âƒ£  Avg Normal Acc       : {avg_norm:.4f}")
    print("-" * 40)
    print(f"2ï¸âƒ£  Avg Bio-Shuffled Acc : {avg_bio_shuf:.4f}")
    print(f"    -> Text Importance   : {avg_norm - avg_text_shuf:.4f} (Drop when Text is broken)")
    print("-" * 40)
    print(f"3ï¸âƒ£  Avg Text-Shuffled Acc: {avg_text_shuf:.4f}")
    print(f"    -> Bio Importance    : {avg_norm - avg_bio_shuf:.4f} (Drop when Bio is broken)")
    
    print("\n[ê²°ë¡  í•´ì„]")
    drop_text_broken = avg_norm - avg_text_shuf
    drop_bio_broken = avg_norm - avg_bio_shuf
    
    if drop_text_broken > drop_bio_broken:
        print("ğŸš¨ ëª¨ë¸ì€ **í…ìŠ¤íŠ¸(Text)** ì •ë³´ì— ë” ë§ì´ ì˜ì¡´í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸš¨ ëª¨ë¸ì€ **ìƒì²´ì‹ í˜¸(Bio)** ì •ë³´ì— ë” ë§ì´ ì˜ì¡´í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

    # ---------------------------------------------------------
    # 5. 3ê°€ì§€ Confusion Matrix ì €ì¥
    # ---------------------------------------------------------
    print("\nGenerating 3 Confusion Matrices (Based on Test Set)...")

    # (1) Normal CM
    save_confusion_matrix(
        data_storage["normal"]["true"], 
        data_storage["normal"]["pred"], 
        f"Confusion Matrix - Normal (Acc: {avg_norm:.4f})",
        "cm_1_normal.png"
    )

    # (2) Bio Shuffled CM
    save_confusion_matrix(
        data_storage["bio_shuffled"]["true"], 
        data_storage["bio_shuffled"]["pred"], 
        f"Confusion Matrix - Bio Shuffled (Text Only Effect)",
        "cm_2_bio_shuffled.png"
    )

    # (3) Text Shuffled CM
    save_confusion_matrix(
        data_storage["text_shuffled"]["true"], 
        data_storage["text_shuffled"]["pred"], 
        f"Confusion Matrix - Text Shuffled (Bio Only Effect)",
        "cm_3_text_shuffled.png"
    )

    print("\n================ DONE =================\n")

if __name__ == "__main__":
    main()