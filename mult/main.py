import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from transformers import BertTokenizer, get_linear_schedule_with_warmup 
import os

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤ import
from config import Config
from dataset import MultimodalDataset
from models import MultimodalFusion
from train import train_one_epoch, validate
from utils import set_seed, save_checkpoint

def main():
    # 1. ì´ˆê¸° ì„¤ì •
    set_seed(42)
    
    # Mac(MPS) ë˜ëŠ” CUDA ì¥ì¹˜ ì„¤ì •
    device = torch.device(Config.DEVICE)
    print(f"ğŸš€ í”„ë¡œì íŠ¸ ì‹œì‘: {Config.PROJECT_NAME}")
    print(f"ğŸ’» ì‚¬ìš© ì¥ì¹˜: {device}") 

    # ì €ì¥ í´ë” ë§Œë“¤ê¸°
    if not os.path.exists("./saved_models"):
        os.makedirs("./saved_models")

    # 2. í† í¬ë‚˜ì´ì € & ë°ì´í„°ì…‹ ë¡œë“œ
    print("\n[1/4] ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
    tokenizer = BertTokenizer.from_pretrained(Config.BERT_MODEL_NAME)
    
    # ì „ì²´ ë°ì´í„° ë¡œë“œ
    full_dataset = MultimodalDataset(Config.DATA_DIR, tokenizer, Config)
    
    # Train/Valid ë¶„í• 
    total_size = len(full_dataset)
    train_size = int(0.8 * total_size)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    print(f"   - ì „ì²´ ë°ì´í„°: {total_size}ê°œ")
    print(f"   - í•™ìŠµìš©(Train): {train_size}ê°œ")
    print(f"   - ê²€ì¦ìš©(Valid): {val_size}ê°œ")
    
    # 3. ë°ì´í„° ë¡œë”
    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=0)
    
    # 4. ëª¨ë¸ ì´ˆê¸°í™”
    print("\n[2/4] ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ (KoBERT + LSTM Fusion)...")
    model = MultimodalFusion(Config).to(device)
    
    # ============================================================
    # [ìˆ˜ì •ë¨] PyTorch ë‚´ì¥ AdamW ì‚¬ìš©
    # ============================================================
    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)
    
    criterion = nn.CrossEntropyLoss()
    
    # 6. í•™ìŠµ ë£¨í”„ ì‹œì‘
    print("\n[3/4] í•™ìŠµ ì‹œì‘!")
    best_val_acc = 0.0
    
    for epoch in range(Config.EPOCHS):
        print(f"\nğŸ“Œ Epoch {epoch+1}/{Config.EPOCHS}")
        
        # Train
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
        print(f"   [Train] Loss: {train_loss:.4f} | Acc: {train_acc*100:.2f}%")
        
        # Validation
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        print(f"   [Valid] Loss: {val_loss:.4f} | Acc: {val_acc*100:.2f}%")
        
        # ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            print(f"   ğŸ‰ ì„±ëŠ¥ ê°±ì‹ ! ({best_val_acc*100:.2f}% -> {val_acc*100:.2f}%) ëª¨ë¸ ì €ì¥í•¨.")
            best_val_acc = val_acc
            save_checkpoint(model, Config.MODEL_SAVE_PATH)
            
    print(f"\n[4/4] ëª¨ë“  í•™ìŠµ ì™„ë£Œ. ìµœì¢… Best Acc: {best_val_acc*100:.2f}%")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {Config.MODEL_SAVE_PATH}")

if __name__ == "__main__":
    main()