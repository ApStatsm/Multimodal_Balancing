# train.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
# ğŸ”¥ [ì¶”ê°€] config ì„í¬íŠ¸
from config import config 

def run_epoch(model, loader, optimizer, device, mode="train"):
    if mode == "train":
        model.train()
    else:
        model.eval()

    # ğŸ”¥ [ì¶”ê°€] Loss ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
    ALPHA = config["balancing"]["alpha"]
    BETA = config["balancing"]["beta"]
    LAMBDA = config["balancing"]["lambda"]
    
    total, correct = 0, 0
    total_loss = 0
    ce = nn.CrossEntropyLoss()
    # ğŸ”¥ [ì¶”ê°€] ì¼ê´€ì„± ì†ì‹¤ì„ ìœ„í•œ L2 ê±°ë¦¬ (MSE)
    mse = nn.MSELoss() 

    loader_pbar = tqdm(loader, desc=f"{mode.upper()}", leave=False)

    with torch.set_grad_enabled(mode == "train"):
        for text_input, bio_input, label in loader_pbar:
            
            for k in text_input:
                text_input[k] = text_input[k].to(device)
            bio_input = bio_input.to(device)
            label = label.to(device)

            if mode == "train":
                optimizer.zero_grad()

            # --- ğŸ”¥ [STEP 1] Full Inputìœ¼ë¡œ í•™ìŠµ (L_CE, L_text, L_bio, h_full ê³„ì‚°) ---
            # MultimodalEndToEndì˜ 4ê°€ì§€ ë°˜í™˜ ê°’ ìˆ˜ì‹ 
            final_logits, h_full, aux_text_logits, aux_bio_logits = model(text_input, bio_input)

            # 1. L_CE (Main Classification Loss)
            loss_ce = ce(final_logits, label)

            # 2. L_Auxiliary (Unimodal Loss)
            loss_text = ce(aux_text_logits, label)
            loss_bio = ce(aux_bio_logits, label)
            
            # --- ğŸ”¥ [STEP 2] Masked Inputìœ¼ë¡œ L_cons ê³„ì‚° ---
            loss_cons = torch.tensor(0.0).to(device) # 0.0 ëŒ€ì‹  í…ì„œë¡œ ì´ˆê¸°í™”
            
            if mode == "train" and LAMBDA > 0:
                # 2-1. í…ìŠ¤íŠ¸ ë§ˆìŠ¤í‚¹ (Bio Only íš¨ê³¼)
                # í…ìŠ¤íŠ¸ inputì˜ ê°’ì„ ì…”í”Œí•˜ì—¬ ë°ì´í„° ê°„ì˜ ì˜ë¯¸ ì—†ëŠ” ì¡°í•©ì„ ë§Œë“¦
                # (LANISTRì˜ ë§ˆìŠ¤í‚¹ ë°©ì‹ ì¤‘ í•˜ë‚˜ì¸ Cross-Modality Shuffling ì ìš©)
                text_input_masked = {k: v.clone() for k, v in text_input.items()}
                idx = torch.randperm(bio_input.size(0)).to(device)
                for k in text_input_masked:
                    text_input_masked[k] = text_input_masked[k][idx] 
                
                # ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸ì™€ ì›ë³¸ ë°”ì´ì˜¤ë¡œ forward ì‹¤í–‰. (ë¡œì§“ì€ ë¬´ì‹œ)
                _, h_text_masked, _, _ = model(text_input_masked, bio_input)
                loss_cons += mse(h_full, h_text_masked) # ì¼ê´€ì„± ì†ì‹¤ 1

                # 2-2. ë°”ì´ì˜¤ ë§ˆìŠ¤í‚¹ (Text Only íš¨ê³¼)
                # ë°”ì´ì˜¤ inputì˜ ê°’ì„ ì…”í”Œ
                bio_input_masked = bio_input.clone()
                idx = torch.randperm(bio_input.size(0)).to(device)
                bio_input_masked = bio_input_masked[idx]
                
                # ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ë§ˆìŠ¤í‚¹ëœ ë°”ì´ì˜¤ë¡œ forward ì‹¤í–‰
                _, h_bio_masked, _, _ = model(text_input, bio_input_masked)
                loss_cons += mse(h_full, h_bio_masked) # ì¼ê´€ì„± ì†ì‹¤ 2


            # --- ğŸ”¥ [STEP 3] ìµœì¢… Total Loss í•©ì‚° ---
            loss = loss_ce + (ALPHA * loss_text + BETA * loss_bio) + (LAMBDA * loss_cons)
            
            if mode == "train":
                loss.backward()
                optimizer.step()

            pred = final_logits.argmax(dim=1)
            correct += (pred == label).sum().item()
            total += label.size(0)
            total_loss += loss.item() # ìµœì¢… í•©ì‚° Lossë¥¼ ê¸°ë¡
            
            # ì§„í–‰ ë°” ì˜†ì— ì‹¤ì‹œê°„ Loss/Acc í‘œì‹œ
            current_acc = correct / total
            loader_pbar.set_postfix({'loss': loss.item(), 'acc': current_acc})

    return correct / total, total_loss / len(loader)


def test_multimodal(model, loader, device, shuffle_mode="none"):
    """
    í…ŒìŠ¤íŠ¸ ëª¨ë“œ: shuffle_modeì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì…”í”Œí•˜ì—¬ í¸í–¥ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    shuffle_mode: "none", "text", "bio"
    Returns: acc, loss, labels, preds, probs (for AUC)
    """
    model.eval()
    total_loss = 0
    ce = nn.CrossEntropyLoss()
    
    all_preds = []
    all_labels = []
    all_probs = []

    # í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œë„ ì§„í–‰ ìƒí™© ë³´ê¸°
    loader_pbar = tqdm(loader, desc=f"TEST({shuffle_mode})", leave=True)

    with torch.no_grad():
        for text_input, bio_input, label in loader_pbar:
            
            for k in text_input:
                text_input[k] = text_input[k].to(device)
            bio_input = bio_input.to(device)
            label = label.to(device)

            # ğŸ² ì…”í”Œ ë¡œì§
            idx = torch.randperm(bio_input.size(0)).to(device)
            if shuffle_mode == "text":
                for k in text_input:
                    text_input[k] = text_input[k][idx]
            elif shuffle_mode == "bio":
                bio_input = bio_input[idx]

            # ğŸ”¥ [ìˆ˜ì •] model(text_input, bio_input)ì˜ 4ê°€ì§€ ë°˜í™˜ê°’ ì¤‘ ìµœì¢… ë¡œì§“ë§Œ ì‚¬ìš©
            # final_logits, h_full, aux_text_logits, aux_bio_logits = model(...)
            final_logits, _, _, _ = model(text_input, bio_input)
            
            loss = ce(final_logits, label)
            total_loss += loss.item()

            # ğŸ”¥ [ìˆ˜ì •] ë‹¤ì¤‘ ë¶„ë¥˜ì´ë¯€ë¡œ [:, 1] ì¸ë±ì‹± ì œê±°
            # ì „ì²´ í™•ë¥  ë¶„í¬ë¥¼ ì €ì¥ (ë‚˜ì¤‘ì— ë¶„ì„í•  ë•Œ í•„ìš”í•˜ë©´ ì‚¬ìš©)
            probs = F.softmax(final_logits, dim=1)
            pred = final_logits.argmax(dim=1)
            
            all_preds.extend(pred.cpu().tolist())
            all_labels.extend(label.cpu().tolist())
            # ë‹¤ì¤‘ ë¶„ë¥˜ í™•ë¥ ê°’ ì €ì¥ (N, 5)
            all_probs.extend(probs.cpu().tolist())

            loader_pbar.set_postfix({'loss': loss.item()})


    from sklearn.metrics import accuracy_score
    acc = accuracy_score(all_labels, all_preds)
    return acc, total_loss / len(loader), all_labels, all_preds, all_probs