# fusion_model.py

import torch
import torch.nn as nn

class CrossAttentionFusion(nn.Module):
    def __init__(self, bio_dim=32, text_dim=768, hidden_dim=256, num_classes=2):
        super().__init__()

        # 1. ì°¨ì› ë§ì¶”ê¸° (Projection)
        self.bio_proj = nn.Linear(bio_dim, hidden_dim)
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        
        # í•™ìŠµ ì•ˆì •í™”ë¥¼ ìœ„í•œ Layer Norm
        self.ln_bio = nn.LayerNorm(hidden_dim)
        self.ln_text = nn.LayerNorm(hidden_dim)

        # 2. Cross Attention
        # (Bio <-> Text ì„œë¡œ ì •ë³´ë¥¼ êµí™˜)
        self.attn = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)

        # 3. ë¶„ë¥˜ê¸° (Classifier)
        # ìœµí•©ëœ ë²¡í„°(Concat)ë¥¼ ë°›ì•„ì„œ ìµœì¢… ì˜ˆì¸¡
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim), # Concat í–ˆìœ¼ë¯€ë¡œ ì…ë ¥ì´ hidden_dim * 2
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_classes)
        )

    # ğŸ”¥ [ìˆ˜ì •] ìœµí•© ì„ë² ë”©(fused_feature)ì„ ì¶”ê°€ë¡œ ë°˜í™˜
    def forward(self, bio, text):
        # [Step 1] Projection & Unsqueeze (Attention ì…ë ¥ì„ ìœ„í•´ ì°¨ì› ì¶”ê°€)
        # (Batch, Dim) -> (Batch, 1, Hidden)
        bio = self.ln_bio(self.bio_proj(bio)).unsqueeze(1)
        text = self.ln_text(self.text_proj(text)).unsqueeze(1)

        # [Step 2] Cross Attention
        # Q(ì§ˆë¬¸): Bio, K/V(ì •ë³´): Text -> Bio ì…ì¥ì—ì„œ Text ì •ë³´ë¥¼ ë°›ìŒ
        attn_output_bio, _ = self.attn(bio, text, text) 
        # Q(ì§ˆë¬¸): Text, K/V(ì •ë³´): Bio -> Text ì…ì¥ì—ì„œ Bio ì •ë³´ë¥¼ ë°›ìŒ
        attn_output_text, _ = self.attn(text, bio, bio) 
        
        # [Step 3] ìœµí•© ë²¡í„° ìƒì„± (Concat)
        # fused_feature: L_cons ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” ìœµí•© ì„ë² ë”©
        fused_feature = torch.cat([
            attn_output_bio.squeeze(1), 
            attn_output_text.squeeze(1)
        ], dim=-1) # (B, Hidden*2)

        # [Step 4] ìµœì¢… ë¶„ë¥˜
        final_logits = self.classifier(fused_feature) # (B, num_classes)

        return final_logits, fused_feature