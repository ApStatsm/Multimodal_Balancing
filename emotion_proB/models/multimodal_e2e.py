# multimodal_e2e.py

import torch.nn as nn
from models.kobert_encoder import KoBERTEncoder
from models.bio_encoder import BioLSTMEncoder
from models.fusion_model import CrossAttentionFusion

class MultimodalEndToEnd(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.text_encoder = KoBERTEncoder()      # frozen
        self.bio_encoder = BioLSTMEncoder(
            input_dim=config["model"]["bio_input_dim"],
            hidden_dim=config["model"]["bio_hidden_dim"],
            output_dim=config["model"]["bio_output_dim"]
        )
        self.fusion = CrossAttentionFusion(
            bio_dim=config["model"]["bio_output_dim"],
            text_dim=768,
            hidden_dim=config["model"]["fusion_hidden_dim"],
            num_classes=config["model"]["num_classes"]
        )

        # ğŸ”¥ [ì¶”ê°€] MAAN ê¸°ë°˜: ë‹¨ì¼ ëª¨ë‹¬ ë³´ì¡° ë¶„ë¥˜ê¸° (L_text, L_bio ê³„ì‚°ìš©)
        num_classes = config["model"]["num_classes"]
        # KoBERT ì¶œë ¥ ì°¨ì›: 768
        self.aux_text_classifier = nn.Linear(768, num_classes) 
        # BioLSTM ì¶œë ¥ ì°¨ì›: 64
        self.aux_bio_classifier = nn.Linear(config["model"]["bio_output_dim"], num_classes) 

    # ğŸ”¥ [ìˆ˜ì •] 4ê°€ì§€ ì¶œë ¥ (final_logits, fused_feature, aux_text_logits, aux_bio_logits)ì„ ë°˜í™˜
    def forward(self, text_input, bio_input):
        # 1. ì¸ì½”ë” ì‹¤í–‰
        text_feat = self.text_encoder(
            text_input["input_ids"],
            text_input["attention_mask"],
            text_input["token_type_ids"]
        )
        bio_feat = self.bio_encoder(bio_input)

        # 2. ë³´ì¡° ë¡œì§“ ê³„ì‚° (L_text, L_bio)
        aux_text_logits = self.aux_text_classifier(text_feat)
        aux_bio_logits = self.aux_bio_classifier(bio_feat)
        
        # 3. ìœµí•© ëª¨ë¸ ì‹¤í–‰ (final_logitsì™€ L_cons ê³„ì‚°ìš© fused_feature ë°˜í™˜)
        final_logits, fused_feature = self.fusion(bio_feat, text_feat)

        # 4. ìµœì¢… ê²°ê³¼ ë°˜í™˜
        return final_logits, fused_feature, aux_text_logits, aux_bio_logits